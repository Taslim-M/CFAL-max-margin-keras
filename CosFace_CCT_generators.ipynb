{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4e2defb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.utils import *\n",
    "\n",
    "from loss.CosFace import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b53ec46f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.applications.vgg19 import VGG19\n",
    "\n",
    "from tensorflow.keras.layers import Input, Lambda, Dense, Flatten\n",
    "from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten, BatchNormalization, Conv2D, MaxPool2D, MaxPooling2D\n",
    "from tensorflow.keras.models import Sequential\n",
    "\n",
    "from keras.callbacks import LearningRateScheduler, Callback,ModelCheckpoint\n",
    "from sklearn.metrics import confusion_matrix, f1_score, precision_score, recall_score\n",
    "\n",
    "\n",
    "from sklearn.metrics import classification_report, precision_score, recall_score, f1_score, accuracy_score, matthews_corrcoef, cohen_kappa_score\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "from sklearn.metrics import average_precision_score\n",
    "\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "from keras import backend as K\n",
    "from tensorflow.keras import layers\n",
    "from keras.regularizers import l2\n",
    "from keras.layers import PReLU\n",
    "from keras import initializers\n",
    "\n",
    "\n",
    "from keras.layers import Input,Conv2D,Activation,Dense,Lambda,Flatten,Embedding,PReLU,BatchNormalization\n",
    "from keras.models import Model\n",
    "import keras.backend as K\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from keras.callbacks import EarlyStopping\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9c4dbf2",
   "metadata": {},
   "source": [
    "# Load KCRC Data (NCT-CRC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6075ac26",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dir = r\"...\\NCT-CRC-HE-100K/\"\n",
    "test_dir = r\"...\\CRC-VAL-HE-7K/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2448786d",
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = ['ADI', 'BACK', 'DEB', 'LYM', 'MUC', 'MUS', 'NORM', 'STR', 'TUM']\n",
    "\n",
    "datagen = ImageDataGenerator(horizontal_flip=True, vertical_flip=True, validation_split=0.15, preprocessing_function=tf.keras.applications.vgg19.preprocess_input)\n",
    "\n",
    "train_batches = datagen.flow_from_directory(directory=train_dir, target_size=(112,112), \n",
    "                                            classes=classes, batch_size=32,subset='training')\n",
    "valid_batches= datagen.flow_from_directory(directory=train_dir, target_size=(112,112), \n",
    "                                           classes=classes, batch_size=32,subset='validation',shuffle=False)\n",
    "\n",
    "test_batches = ImageDataGenerator(preprocessing_function=tf.keras.applications.vgg19.preprocess_input).flow_from_directory(directory=test_dir, target_size=(112,112), classes=classes, batch_size=32, shuffle=False)\n",
    "\n",
    "weights_kcrc = [10407, 10566, 11512, 11557, 8896, 13536, 8763, 10446, 14317]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c6414e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_effective_weights(counts, beta=0.999):\n",
    "    effective_num = 1.0 - np.power(beta, counts)\n",
    "    weights = (1.0 - beta) / np.array(effective_num)\n",
    "    weights = weights / np.sum(weights) * int(len(counts))\n",
    "    return weights\n",
    "\n",
    "eff_weights = calc_effective_weights(weights_kcrc, beta=0.9999)\n",
    "print(eff_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5689942",
   "metadata": {},
   "source": [
    "# Define Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79627652",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers\n",
    "from tensorflow import keras\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow_addons as tfa\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f926afa",
   "metadata": {},
   "outputs": [],
   "source": [
    "positional_emb = True\n",
    "conv_layers = 2\n",
    "projection_dim = 128\n",
    "\n",
    "num_heads = 2\n",
    "transformer_units = [\n",
    "    projection_dim,\n",
    "    projection_dim,\n",
    "]\n",
    "transformer_layers = 2\n",
    "stochastic_depth_rate = 0.1\n",
    "\n",
    "learning_rate = 0.001\n",
    "weight_decay = 0.0001\n",
    "batch_size = 128\n",
    "image_size = 112"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca2e4068",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 10\n",
    "num_classes = 9 \n",
    "input_shape = (112, 112, 3) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db4ea1d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CCTTokenizer(layers.Layer):\n",
    "    def __init__(\n",
    "        self,\n",
    "        kernel_size=3,\n",
    "        stride=1,\n",
    "        padding=1,\n",
    "        pooling_kernel_size=3,\n",
    "        pooling_stride=2,\n",
    "        num_conv_layers=conv_layers,\n",
    "        num_output_channels=[64, 128],\n",
    "        positional_emb=positional_emb,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        super(CCTTokenizer, self).__init__(**kwargs)\n",
    "\n",
    "        # This is our tokenizer.\n",
    "        self.conv_model = keras.Sequential()\n",
    "        for i in range(num_conv_layers):\n",
    "            self.conv_model.add(\n",
    "                layers.Conv2D(\n",
    "                    num_output_channels[i],\n",
    "                    kernel_size,\n",
    "                    stride,\n",
    "                    padding=\"valid\",\n",
    "                    use_bias=False,\n",
    "                    activation=\"relu\",\n",
    "                    kernel_initializer=\"he_normal\",\n",
    "                )\n",
    "            )\n",
    "            self.conv_model.add(layers.ZeroPadding2D(padding))\n",
    "            self.conv_model.add(\n",
    "                layers.MaxPool2D(pooling_kernel_size, pooling_stride, \"same\")\n",
    "            )\n",
    "\n",
    "        self.positional_emb = positional_emb\n",
    "\n",
    "    def call(self, images):\n",
    "        outputs = self.conv_model(images)\n",
    "        # After passing the images through our mini-network the spatial dimensions\n",
    "        # are flattened to form sequences.\n",
    "        reshaped = tf.reshape(\n",
    "            outputs,\n",
    "            (-1, tf.shape(outputs)[1] * tf.shape(outputs)[2], tf.shape(outputs)[-1]),\n",
    "        )\n",
    "        return reshaped\n",
    "\n",
    "    def positional_embedding(self, image_size):\n",
    "        # Positional embeddings are optional in CCT. Here, we calculate\n",
    "        # the number of sequences and initialize an `Embedding` layer to\n",
    "        # compute the positional embeddings later.\n",
    "        if self.positional_emb:\n",
    "            dummy_inputs = tf.ones((1, image_size, image_size, 3))\n",
    "            dummy_outputs = self.call(dummy_inputs)\n",
    "            sequence_length = tf.shape(dummy_outputs)[1]\n",
    "            projection_dim = tf.shape(dummy_outputs)[-1]\n",
    "\n",
    "            embed_layer = layers.Embedding(\n",
    "                input_dim=sequence_length, output_dim=projection_dim\n",
    "            )\n",
    "            return embed_layer, sequence_length\n",
    "        else:\n",
    "            return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ec4e3bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Referred from: github.com:rwightman/pytorch-image-models.\n",
    "class StochasticDepth(layers.Layer):\n",
    "    def __init__(self, drop_prop, **kwargs):\n",
    "        super(StochasticDepth, self).__init__(**kwargs)\n",
    "        self.drop_prob = drop_prop\n",
    "\n",
    "    def call(self, x, training=None):\n",
    "        if training:\n",
    "            keep_prob = 1 - self.drop_prob\n",
    "            shape = (tf.shape(x)[0],) + (1,) * (tf.shape(x).shape[0] - 1)\n",
    "            random_tensor = keep_prob + tf.random.uniform(shape, 0, 1)\n",
    "            random_tensor = tf.floor(random_tensor)\n",
    "            return (x / keep_prob) * random_tensor\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7671d8f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mlp(x, hidden_units, dropout_rate):\n",
    "    for units in hidden_units:\n",
    "        x = layers.Dense(units, activation=tf.nn.gelu)(x)\n",
    "        x = layers.Dropout(dropout_rate)(x)\n",
    "    return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ad40281",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_cct_model(\n",
    "    image_size=image_size,\n",
    "    input_shape=input_shape,\n",
    "    num_heads=num_heads,\n",
    "    projection_dim=projection_dim,\n",
    "    transformer_units=transformer_units,\n",
    "):\n",
    "\n",
    "    inputs = layers.Input(input_shape)\n",
    "    label = Input(shape=(9,))\n",
    "\n",
    "    # Encode patches.\n",
    "    cct_tokenizer = CCTTokenizer()\n",
    "    encoded_patches = cct_tokenizer(inputs)\n",
    "\n",
    "    # Apply positional embedding.\n",
    "    if positional_emb:\n",
    "        pos_embed, seq_length = cct_tokenizer.positional_embedding(image_size)\n",
    "        positions = tf.range(start=0, limit=seq_length, delta=1)\n",
    "        position_embeddings = pos_embed(positions)\n",
    "        encoded_patches += position_embeddings\n",
    "\n",
    "    # Calculate Stochastic Depth probabilities.\n",
    "    dpr = [x for x in np.linspace(0, stochastic_depth_rate, transformer_layers)]\n",
    "\n",
    "    # Create multiple layers of the Transformer block.\n",
    "    for i in range(transformer_layers):\n",
    "        # Layer normalization 1.\n",
    "        x1 = layers.LayerNormalization(epsilon=1e-5)(encoded_patches)\n",
    "\n",
    "        # Create a multi-head attention layer.\n",
    "        attention_output = layers.MultiHeadAttention(\n",
    "            num_heads=num_heads, key_dim=projection_dim, dropout=0.1\n",
    "        )(x1, x1)\n",
    "\n",
    "        # Skip connection 1.\n",
    "        attention_output = StochasticDepth(dpr[i])(attention_output)\n",
    "        x2 = layers.Add()([attention_output, encoded_patches])\n",
    "\n",
    "        # Layer normalization 2.\n",
    "        x3 = layers.LayerNormalization(epsilon=1e-5)(x2)\n",
    "\n",
    "        # MLP.\n",
    "        x3 = mlp(x3, hidden_units=transformer_units, dropout_rate=0.1)\n",
    "\n",
    "        # Skip connection 2.\n",
    "        x3 = StochasticDepth(dpr[i])(x3)\n",
    "        encoded_patches = layers.Add()([x3, x2])\n",
    "\n",
    "    # Apply sequence pooling.\n",
    "    representation = layers.LayerNormalization(epsilon=1e-5)(encoded_patches)\n",
    "    attention_weights = tf.nn.softmax(layers.Dense(1)(representation), axis=1)\n",
    "    weighted_representation = tf.matmul(\n",
    "        attention_weights, representation, transpose_a=True\n",
    "    )\n",
    "    weighted_representation = tf.squeeze(weighted_representation, -2)\n",
    "\n",
    "    # Classify outputs.\n",
    "    #logits = layers.Dense(num_classes, activation='softmax')(weighted_representation)\n",
    "    output = CosFace(n_classes=9, s=11.0, m=0.1)([weighted_representation, label])\n",
    "    # Create the Keras model.\n",
    "    model = keras.Model(inputs=[inputs, label], outputs=output)\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3614aef4",
   "metadata": {},
   "outputs": [],
   "source": [
    "cct_model = create_cct_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34f877cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_filepath = \"/content/checkpoint\"\n",
    "\n",
    "def run_experiment(model):\n",
    "    optimizer = tfa.optimizers.AdamW(learning_rate=0.001, weight_decay=0.0001)\n",
    "\n",
    "    loss_weighted=weighted_categorical_crossentropy(eff_weights)\n",
    "    model.compile(\n",
    "        optimizer=optimizer,\n",
    "        loss=loss_weighted,\n",
    "        metrics=[\n",
    "            keras.metrics.CategoricalAccuracy(name=\"accuracy\"),\n",
    "        ],\n",
    "    )\n",
    "    datagen_to_use = train_batches\n",
    "    steps_per_epoch = len(datagen_to_use) \n",
    "\n",
    "    train_acc_metric = tf.keras.metrics.Accuracy()\n",
    "    val_acc_metric = tf.keras.metrics.Accuracy()\n",
    "    max_f1 = 0\n",
    "    for epoch in range(num_epochs):\n",
    "        print(\"\\nStart of epoch %d\" % (epoch,))\n",
    "      # Iterate over the batches of the dataset.\n",
    "        for step, (x_batch_train, y_batch_train) in enumerate(datagen_to_use):\n",
    "            x_train = x_test = x_batch_train\n",
    "            y_train = y_test = y_batch_train\n",
    "\n",
    "            model.train_on_batch(x=[x_train,y_train],y=y_train)\n",
    "\n",
    "            if step >= steps_per_epoch:  # manually detect the end of the epoch\n",
    "                break \n",
    "\n",
    "          #print(\"Training acc over epoch: %.4f\" % (float(train_acc),))\n",
    "        y_test, y_pred = get_generator_output(valid_batches, model) # get validation score\n",
    "        _ = train_acc_metric.update_state(y_test, y_pred)\n",
    "        #print(classification_report(y_test, y_pred))\n",
    "    return None\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08d2fdbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_experiment(cct_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "116a51b2",
   "metadata": {},
   "source": [
    "# Load Best Model and Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fd90c84",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = ''\n",
    "cct_model.load_weights(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9568d305",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test, y_pred = get_generator_output(test_batches, cct_model)\n",
    "print(classification_report(y_test, y_pred,digits=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63af3e74",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
