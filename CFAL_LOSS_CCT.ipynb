{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4e2defb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from models import *\n",
    "from utils import *\n",
    "\n",
    "from cfal_loss import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b53ec46f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.applications.vgg19 import VGG19\n",
    "\n",
    "from tensorflow.keras.layers import Input, Lambda, Dense, Flatten\n",
    "from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten, BatchNormalization, Conv2D, MaxPool2D, MaxPooling2D\n",
    "from tensorflow.keras.models import Sequential\n",
    "\n",
    "from keras.callbacks import LearningRateScheduler, Callback,ModelCheckpoint\n",
    "from sklearn.metrics import confusion_matrix, f1_score, precision_score, recall_score\n",
    "\n",
    "\n",
    "from sklearn.metrics import classification_report, precision_score, recall_score, f1_score, accuracy_score, matthews_corrcoef, cohen_kappa_score\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "from sklearn.metrics import average_precision_score\n",
    "\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "from keras import backend as K\n",
    "from tensorflow.keras import layers\n",
    "from keras.regularizers import l2\n",
    "from keras.layers import PReLU\n",
    "from keras import initializers\n",
    "\n",
    "\n",
    "from keras.layers import Input,Conv2D,Activation,Dense,Lambda,Flatten,Embedding,PReLU,BatchNormalization\n",
    "from keras.models import Model\n",
    "import keras.backend as K\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from keras.callbacks import EarlyStopping\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9c4dbf2",
   "metadata": {},
   "source": [
    "# Load KCRC Data (NCT-CRC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6075ac26",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dir = r\"C:\\data\\NCT-CRC-HE-100K/\"\n",
    "test_dir = r\"C:\\data\\CRC-VAL-HE-7K/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2448786d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add an extra class for the RW of the CFAL LOSS\n",
    "classes = ['ADI', 'BACK', 'DEB', 'LYM', 'MUC', 'MUS', 'NORM', 'STR', 'TUM','None']\n",
    "\n",
    "datagen = ImageDataGenerator(horizontal_flip=True, vertical_flip=True, validation_split=0.15, preprocessing_function=tf.keras.applications.vgg19.preprocess_input)\n",
    "\n",
    "train_batches = datagen.flow_from_directory(directory=train_dir, target_size=(112,112), \n",
    "                                            classes=classes, batch_size=32,subset='training')\n",
    "valid_batches= datagen.flow_from_directory(directory=train_dir, target_size=(112,112), \n",
    "                                           classes=classes, batch_size=32,subset='validation',shuffle=False)\n",
    "\n",
    "test_batches = ImageDataGenerator(preprocessing_function=tf.keras.applications.vgg19.preprocess_input).flow_from_directory(directory=test_dir, target_size=(112,112), classes=classes, batch_size=32, shuffle=False)\n",
    "\n",
    "weights_kcrc = [10407, 10566, 11512, 11557, 8896, 13536, 8763, 10446, 14317]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c6414e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_effective_weights(counts, beta=0.999):\n",
    "    effective_num = 1.0 - np.power(beta, counts)\n",
    "    weights = (1.0 - beta) / np.array(effective_num)\n",
    "    weights = weights / np.sum(weights) * int(len(counts))\n",
    "    return weights\n",
    "\n",
    "eff_weights = calc_effective_weights(weights_kcrc, beta=0.9999)\n",
    "print(eff_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5689942",
   "metadata": {},
   "source": [
    "# Define Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79627652",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers\n",
    "from tensorflow import keras\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow_addons as tfa\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f926afa",
   "metadata": {},
   "outputs": [],
   "source": [
    "positional_emb = True\n",
    "conv_layers = 2\n",
    "projection_dim = 128\n",
    "\n",
    "num_heads = 2\n",
    "transformer_units = [\n",
    "    projection_dim,\n",
    "    projection_dim,\n",
    "]\n",
    "transformer_layers = 2\n",
    "stochastic_depth_rate = 0.1\n",
    "\n",
    "learning_rate = 0.001\n",
    "weight_decay = 0.0001\n",
    "batch_size = 128\n",
    "num_epochs = 35\n",
    "image_size = 112"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca2e4068",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 10 # one additional\n",
    "input_shape = (112, 112, 3) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ad40281",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_cct_model(\n",
    "    image_size=image_size,\n",
    "    input_shape=input_shape,\n",
    "    num_heads=num_heads,\n",
    "    projection_dim=projection_dim,\n",
    "    transformer_units=transformer_units,\n",
    "):\n",
    "\n",
    "    inputs = layers.Input(input_shape)\n",
    "\n",
    "    # Encode patches.\n",
    "    cct_tokenizer = CCTTokenizer(num_conv_layers=conv_layers,positional_emb=positional_emb)\n",
    "    encoded_patches = cct_tokenizer(inputs)\n",
    "\n",
    "    # Apply positional embedding.\n",
    "    if positional_emb:\n",
    "        pos_embed, seq_length = cct_tokenizer.positional_embedding(image_size)\n",
    "        positions = tf.range(start=0, limit=seq_length, delta=1)\n",
    "        position_embeddings = pos_embed(positions)\n",
    "        encoded_patches += position_embeddings\n",
    "\n",
    "    # Calculate Stochastic Depth probabilities.\n",
    "    dpr = [x for x in np.linspace(0, stochastic_depth_rate, transformer_layers)]\n",
    "\n",
    "    # Create multiple layers of the Transformer block.\n",
    "    for i in range(transformer_layers):\n",
    "        # Layer normalization 1.\n",
    "        x1 = layers.LayerNormalization(epsilon=1e-5)(encoded_patches)\n",
    "\n",
    "        # Create a multi-head attention layer.\n",
    "        attention_output = layers.MultiHeadAttention(\n",
    "            num_heads=num_heads, key_dim=projection_dim, dropout=0.1\n",
    "        )(x1, x1)\n",
    "\n",
    "        # Skip connection 1.\n",
    "        attention_output = StochasticDepth(dpr[i])(attention_output)\n",
    "        x2 = layers.Add()([attention_output, encoded_patches])\n",
    "\n",
    "        # Layer normalization 2.\n",
    "        x3 = layers.LayerNormalization(epsilon=1e-5)(x2)\n",
    "\n",
    "        # MLP.\n",
    "        x3 = mlp(x3, hidden_units=transformer_units, dropout_rate=0.1)\n",
    "\n",
    "        # Skip connection 2.\n",
    "        x3 = StochasticDepth(dpr[i])(x3)\n",
    "        encoded_patches = layers.Add()([x3, x2])\n",
    "\n",
    "    # Apply sequence pooling.\n",
    "    representation = layers.LayerNormalization(epsilon=1e-5)(encoded_patches)\n",
    "    attention_weights = tf.nn.softmax(layers.Dense(1)(representation), axis=1)\n",
    "    weighted_representation = tf.matmul(\n",
    "        attention_weights, representation, transpose_a=True\n",
    "    )\n",
    "    weighted_representation = tf.squeeze(weighted_representation, -2)\n",
    "\n",
    "    # Classify outputs.\n",
    "  \n",
    "    logits = ClusteringAffinity(9, 1, 130)(weighted_representation)\n",
    "    # Create the Keras model.\n",
    "    model = keras.Model(inputs=inputs, outputs=logits)\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3614aef4",
   "metadata": {},
   "outputs": [],
   "source": [
    "cct_model = create_cct_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acd78057",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 1\n",
    "history = run_experiment(cct_model, train_batches, valid_batches, batch_size , num_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "116a51b2",
   "metadata": {},
   "source": [
    "# Load Best Model and Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fd90c84",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = ''\n",
    "cct_model.load_weights(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9568d305",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_pred = cct_model.predict(test_batches)\n",
    "y_pred = np.argmax(Y_pred ,axis =1)\n",
    "y_true = test_batches.classes\n",
    "print(classification_report(y_true, y_pred,digits=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b0fb8ed",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
